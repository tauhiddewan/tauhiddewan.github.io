---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
---


<!-- ### A Transformer-in-Transformer Network Utilizing Knowledge Distillation for Image Recognition -->
**First Author** (Under Peer Review)

* The featured transformer-in-transformer architecture with knowledge distillation enables simultaneous exploration of local and global features in natural images, facilitating faster learning from the teacher model while minimizing resource requirements.

* A novel loss function was developed to harmonize teacher and student losses, addressing the unique characteristics of hybrid-labeled images and enhancing the proposed model's effectiveness.

* Rigorous evaluations conducted across MNIST, CIFAR10, and CIFAR100 datasets substantiate the effectiveness of the proposed approach. The empirical validation reveals noteworthy improvements in execution speed and accuracy, firmly establishing a performance benchmark.


[<img src="https://img.icons8.com/emoji/24/000000/up-arrow-emoji.png"/>](https://tauhiddewan.github.io/research/#)[Top](https://tauhiddewan.github.io/research/#)